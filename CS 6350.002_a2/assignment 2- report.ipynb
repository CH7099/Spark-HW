{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f138fe4a",
   "metadata": {},
   "source": [
    "# Assignment 2-1: Friend Recommendation using Mutual Friends\n",
    "**Course**: CS6350.002 Big Data Management & Analytics  \n",
    "**Team Members**:  \n",
    "- Yoonkyung Lee (NetID: yxl240011)  \n",
    "- Changhui Sun (NetID: cxs240024)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255b036d",
   "metadata": {},
   "source": [
    "## 1. Algorithm  \n",
    "The first step is to load and clean the data, keeping only the lines that contain a tab character. Then, we parse and explode each line to obtain a DataFrame with two columns: user and friend.  \n",
    "Next, we aim to find all pairs of mutual friends for a given user A. To do this, we transform the data so that each user A produces pairs such as [(B, C), (B, D), ...], meaning that B and C are mutual friends of A.  \n",
    "After generating these pairs, we remove those pairs that are already directly connected in the original friendship list. For example, if B and C are both friends of A, but B and C are already direct friends with each other, that pair will be excluded from the recommendation set.  \n",
    "The next step is to count the number of mutual friends for each remaining pair. If two users share more mutual friends, the score of recommendation will high.\n",
    "Once the mutual-friend counts are computed, we need to turned the relationship into two directional recommendations. Ensures that both users receive each other as potential friend suggestions.\n",
    "Then, for each user, we rank all recommended friends by descending mutual-friend count and keep the top 10 recommendations per user.  \n",
    "(In my real code, I randomly select 10 users as a sample because using the entire dataset would cause memory issues during the aggregation step.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af49301",
   "metadata": {},
   "source": [
    "## 2. Pseudo-code  \n",
    "#### Load & Clean  \n",
    "1  \n",
    "Input: text file soc-LiveJournal1Adj.txt  \n",
    "Read all lines into dataframe df\n",
    "Keep only rows that contain a tab character\n",
    "\n",
    "#### Parse & Explode \n",
    "2  \n",
    "For each row in df:  \n",
    "    Split the line by tab into (user, friends_string)  \n",
    "    Split friends_string by commas to get friend_list  \n",
    "Create dataframe (user, friend) by exploding friend_list  \n",
    "\n",
    "#### Friend-of-friend pairs    \n",
    "3  \n",
    "For each user:  \n",
    "    For every pair of friends (f1, f2):  \n",
    "        If f1 < f2:  \n",
    "            Record pair (f1, f2) with mutualFriend = user  \n",
    "\n",
    "#### Remove direct friends from above  \n",
    "4  \n",
    "Create set of all direct friendships (user, friend)   \n",
    "Remove any (f1, f2) pairs that already exist in the direct friend set  \n",
    "\n",
    "#### Count mutual friends    \n",
    "5  \n",
    "Group all remaining pairs (f1, f2)  \n",
    "Count the number of distinct mutualFriend values for each pair  \n",
    "\n",
    "#### Create directional recommendations   \n",
    "6  \n",
    "For each pair (f1, f2, mutualCount):  \n",
    "    Output two directed records:  \n",
    "        (src = f1, dst = f2, mutualCount)  \n",
    "        (src = f2, dst = f1, mutualCount)  \n",
    "\n",
    "#### Rank and keep top 10 recommendations  \n",
    "7  \n",
    "For each user src:  \n",
    "    Sort recommended users dst by:  \n",
    "        1. mutualCount descending  \n",
    "        2. dst ascending (tie-breaking)  \n",
    "    Keep the first 10 entries  \n",
    "\n",
    "#### Format and output  \n",
    "8  \n",
    "For each user src:  \n",
    "    Combine their top 10 dst IDs into one comma-separated list  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd423917",
   "metadata": {},
   "source": [
    "## 2. Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7678b00",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import subprocess\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import split, col, explode, least, greatest\n",
    "from pyspark.sql.window import Window\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"MutualFriends\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"6g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "subprocess.run(\n",
    "    [\"wget\", \"https://an-ml.s3.us-west-1.amazonaws.com/soc-LiveJournal1Adj.txt\"], check=True\n",
    ")\n",
    "df = spark.read.text(\"soc-LiveJournal1Adj.txt\")\n",
    "df.show()\n",
    "dfClean = df.filter(col(\"value\").contains(\"\\t\"))\n",
    "df = (dfClean.withColumn(\"user\", split(col(\"value\"), \"\\\\t\").getItem(0)).withColumn(\"friends\", split(col(\"value\"), \"\\\\t\").getItem(1)). withColumn(\"friend\", split(col(\"friends\"), \",\")).select(\"user\", \"friend\"))\n",
    "df.show()\n",
    "\n",
    "dfExploded = df.withColumn(\"friendSigle\", explode(col(\"friend\")))\n",
    "\n",
    "targets = (\n",
    "    dfExploded.select(col(\"user\").cast(\"int\").alias(\"user\")).distinct().orderBy(F.rand()).limit(10))\n",
    "\n",
    "dfTargets = dfExploded.join(targets, on=\"user\", how=\"inner\")\n",
    "\n",
    "dfPair = dfTargets.alias(\"a\").join(dfTargets.alias(\"b\"), on=\"user\") \\\n",
    "    .where(col(\"a.friendSigle\") < col(\"b.friendSigle\")) \\\n",
    "    .select(col(\"user\").alias(\"mutualFriend\"),\n",
    "            col(\"a.friendSigle\").alias(\"f1\"),\n",
    "            col(\"b.friendSigle\").alias(\"f2\"))\n",
    "\n",
    "\n",
    "dfDirect = dfTargets.select(col(\"user\").alias(\"df1\"), col(\"friendSigle\").alias(\"df2\"))\n",
    "\n",
    "\n",
    "dfDirectNorm = dfDirect.select(least(col(\"df1\").cast(\"int\"), col(\"df2\").cast(\"int\")).alias(\"f1\"), greatest(col(\"df1\").cast(\"int\"), col(\"df2\").cast(\"int\")).alias(\"f2\")).distinct()\n",
    "\n",
    "dfDirectNorm.filter(col(\"f1\") > col(\"f2\")).count()\n",
    "dfPair.filter(col(\"f1\") > col(\"f2\")).count()\n",
    "\n",
    "dfNoDirect = dfPair.join(dfDirectNorm, on=[\"f1\", \"f2\"], how=\"left_anti\")\n",
    "\n",
    "dfMutualCount = (dfNoDirect.select(\"f1\",\"f2\",\"mutualFriend\").distinct().groupBy(\"f1\",\"f2\").agg(F.countDistinct(\"mutualFriend\").alias(\"mutualCount\")))\n",
    "\n",
    "# directed recommendation\n",
    "a = dfMutualCount.select(F.col(\"f1\").alias(\"src\"), F.col(\"f2\").alias(\"dst\"), \"mutualCount\")\n",
    "b = dfMutualCount.select(F.col(\"f2\").alias(\"src\"), F.col(\"f1\").alias(\"dst\"), \"mutualCount\")\n",
    "directed = a.unionByName(b)\n",
    "\n",
    "w = Window.partitionBy(\"src\").orderBy(F.col(\"mutualCount\").desc(), F.col(\"dst\").asc())\n",
    "top10 = directed.withColumn(\"rk\", F.row_number().over(w)).where(F.col(\"rk\") <= 10)\n",
    "\n",
    "# format\n",
    "out = (\n",
    "    top10.groupBy(\"src\")\n",
    "    .agg(F.collect_list(F.struct(\"rk\", \"dst\")).alias(\"lst\"))\n",
    "    .select(\n",
    "        \"src\",\n",
    "        F.expr(\"concat_ws(',', transform(array_sort(lst), x -> cast(x.dst as string)))\").alias(\"top10\")\n",
    "    )\n",
    ")\n",
    "\n",
    "sample10 = out.orderBy(F.rand()).limit(10)\n",
    "sample10.show(truncate=False)\n",
    "lines = sample10.selectExpr(\"concat(cast(src as string), '\\t', top10) as line\")\n",
    "with open(\"q1_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for row in lines.toLocalIterator():\n",
    "        f.write(row[\"line\"] + \"\\n\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50cbe18",
   "metadata": {},
   "source": [
    "## 3. Output  \n",
    "6958\t32743,32744,32749,32826,32838,32839,33443,34573  \n",
    "42575\t42573,42577,42584,42585,42587,42588,42608  \n",
    "10350\t10906,14432,19109,25327,33517,40937,7427,771  \n",
    "771\t 10350,10906,14432,19109,25327,33517,40937,7427  \n",
    "32749\t32743,32744,32826,32838,32839,33443,34573,6958  \n",
    "23786\t23717,23753,32557,49577  \n",
    "42585\t42573,42575,42577,42584,42587,42588,42608  \n",
    "42584\t42573,42575,42577,42585,42587,42588,42608  \n",
    "10906\t10350,14432,19109,25327,33517,40937,7427,771  \n",
    "18862\t24140,24240,24246,29592,29612,7293,7444  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be147e65",
   "metadata": {},
   "source": [
    "# Assignment 2-2: Implementing Naive Bayes Classifier using Spark MapReduce\n",
    "\n",
    "**Course**: CS6350.002 Big Data Management & Analytics  \n",
    "**Team Members**:  \n",
    "- Yoonkyung Lee (NetID: yxl240011)  \n",
    "- Changhui Sun (NetID: cxs240024)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Dataset Description\n",
    "\n",
    "We used the **SMS Spam Collection Dataset** from the UCI Machine Learning Repository:  \n",
    "https://archive.ics.uci.edu/dataset/228/sms+spam+collection\n",
    "\n",
    "- Total number of samples: **4503**\n",
    "- Label distribution:\n",
    "  - **Ham**: 3903\n",
    "  - **Spam**: 600\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Pseudo-code of Naive Bayes using MapReduce in PySpark\n",
    "\n",
    "### **Training Phase**\n",
    "\n",
    "#### Map\n",
    "```python\n",
    "(trainingData: RDD[(label, SparseVector)])\n",
    ".map(lambda x: (label, DenseVector(x[1].toArray())))\n",
    "```\n",
    "\n",
    "#### Reduce\n",
    "```python\n",
    ".reduceByKey(lambda x, y: x + y)\n",
    "```\n",
    "\n",
    "#### Conditional Probabilities\n",
    "```python\n",
    "P(w_i | C) = (count(w_i in class C) + 1) / (totalWordsInClass + VocabSize)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Testing Phase**\n",
    "\n",
    "For each test message:\n",
    "1. Initialize:\n",
    "   ```python\n",
    "   logProbHam = log(P(ham))\n",
    "   logProbSpam = log(P(spam))\n",
    "   ```\n",
    "2. For each word `w_i` with frequency `f_i` in message:\n",
    "   ```python\n",
    "   logProbHam += f_i * log(P(w_i | ham))\n",
    "   logProbSpam += f_i * log(P(w_i | spam))\n",
    "   ```\n",
    "3. Choose label:\n",
    "   ```python\n",
    "   predicted = argmax(logProbHam, logProbSpam)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Summary of Results\n",
    "\n",
    "- **Vocabulary Size**: 13423\n",
    "- **P(spam)**: 0.1332\n",
    "- **P(ham)**: 0.8668\n",
    "- **Accuracy**: **97.48%** (1044 correct / 1071 total)\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Notes on Implementation\n",
    "\n",
    "- We implemented the Naive Bayes classifier **entirely from scratch**, using **RDD transformations** (`map`, `reduceByKey`) to calculate class priors and conditional probabilities.\n",
    "- We used `CountVectorizer` from `pyspark.ml` to tokenize and vectorize the text.\n",
    "- `DenseVector` was used internally to simplify summation of word counts per class.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. File List\n",
    "\n",
    "- `q2.py`: Full implementation code.\n",
    "- `naive_bayes_report.md`: This report.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
